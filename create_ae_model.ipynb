{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:25:16.285646Z",
     "start_time": "2024-10-30T08:25:07.361915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "c9272ced716478d7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. 資料加載與標記",
   "id": "a6fb182aae4eb89"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-30T08:25:16.316748Z",
     "start_time": "2024-10-30T08:25:16.298280Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "from data_preprocess import extract_features\n",
    "\n",
    "def load_normal_files(directory):\n",
    "    \"\"\"\n",
    "    載入目錄中所有正常（no-leak）音訊檔案。\n",
    "\n",
    "    Args:\n",
    "        directory (str): 音訊檔案所在的目錄。\n",
    "\n",
    "    Returns:\n",
    "        list: 正常音訊資料的列表，每個元素為 (音訊資料, 取樣率, 檔案名稱)。\n",
    "    \"\"\"\n",
    "    normal_files = []\n",
    "    for dir in os.listdir(directory):\n",
    "        if 'no-leak' not in dir:\n",
    "            continue  # 只處理包含 'no-leak' 的目錄\n",
    "        \n",
    "        dir_path = os.path.join(directory, dir)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            continue  # 確保是目錄\n",
    "        \n",
    "        for file in os.listdir(dir_path):\n",
    "            if not file.endswith(\".wav\"):\n",
    "                continue  # 只處理 .wav 檔案\n",
    "            file_path = os.path.join(dir_path, file)\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            filename = f'{dir}_{file}'\n",
    "            normal_files.append((y, sr, filename))\n",
    "    \n",
    "    return normal_files\n",
    "\n",
    "def create_normal_dataset(directory, desired_time=2.0, n_mels=128, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    載入並預處理正常（no-leak）音訊資料。\n",
    "\n",
    "    Args:\n",
    "        directory (str): 音訊檔案所在的目錄。\n",
    "        desired_time (float): 每個樣本的目標時間長度（秒）。\n",
    "        n_mels (int): Mel 頻帶數。\n",
    "        n_fft (int): FFT 大小。\n",
    "        hop_length (int): hop length。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 預處理後的特徵資料，形狀為 (樣本數, max_len, n_mels, 1)。\n",
    "    \"\"\"\n",
    "    normal_files = load_normal_files(directory)\n",
    "    \n",
    "    print(f\"總正常樣本數: {len(normal_files)}\")\n",
    "    \n",
    "    features = []\n",
    "    for audio, sr, filename in normal_files:\n",
    "        feature = extract_features(audio, sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, desired_time=desired_time)\n",
    "        features.append(feature)\n",
    "    \n",
    "    X = np.array(features)\n",
    "    print(f\"特徵形狀（未正規化）: {X.shape}\")  # 預期形狀: (樣本數, max_len, n_mels)\n",
    "    \n",
    "    # 正規化特徵\n",
    "    X = (X - np.mean(X)) / np.std(X)\n",
    "    \n",
    "    # 擴展維度以符合 Autoencoder 輸入 (樣本數, max_len, n_mels, 1)\n",
    "    X = np.expand_dims(X, -1)\n",
    "    print(f\"特徵形狀（擴展後）: {X.shape}\")  # 預期形狀: (樣本數, max_len, n_mels, 1)\n",
    "    \n",
    "    return X\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. 建立 Autoencoder 模型\n",
   "id": "55e34253937c671f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:43:30.911773Z",
     "start_time": "2024-10-30T08:43:30.899696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.layers import Flatten, Dense, Reshape\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    mae_loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "    return mse_loss + mae_loss\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "def create_autoencoder(input_shape):\n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    # 編碼器\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.3)(encoded)\n",
    "    \n",
    "    # Flatten and Dense layers at bottleneck\n",
    "    x = Flatten()(encoded)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(encoded.shape[1] * encoded.shape[2] * encoded.shape[3], activation='relu')(x)\n",
    "    x = Reshape((encoded.shape[1], encoded.shape[2], encoded.shape[3]))(x)\n",
    "\n",
    "    # 解碼器\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='linear', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "e5d13120a6048b8e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. 訓練模型",
   "id": "d1041d396f002ca5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:43:34.093417Z",
     "start_time": "2024-10-30T08:43:34.085418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def train_autoencoder(autoencoder, X_train, epochs=50, batch_size=32, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    訓練 Autoencoder 模型。\n",
    "\n",
    "    Args:\n",
    "        autoencoder (tensorflow.keras.Model): 已建立的 Autoencoder 模型。\n",
    "        X_train (np.ndarray): 訓練資料，形狀為 (樣本數, max_len, n_mels, 1)。\n",
    "        epochs (int): 訓練輪數。\n",
    "        batch_size (int): 批次大小。\n",
    "        validation_split (float): 驗證集比例。\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.callbacks.History: 訓練歷史紀錄。\n",
    "    \"\"\"\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "    history = autoencoder.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_split=validation_split,\n",
    "        verbose=2,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "    return history"
   ],
   "id": "caa60b879d21434a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. 繪製訓練過程",
   "id": "528f1603853e0013"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:43:36.163684Z",
     "start_time": "2024-10-30T08:43:36.159366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    繪製訓練過程中的損失曲線。\n",
    "\n",
    "    Args:\n",
    "        history (tensorflow.keras.callbacks.History): 訓練歷史紀錄。\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='訓練損失')\n",
    "    plt.plot(history.history['val_loss'], label='驗證損失')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Autoencoder 訓練過程')\n",
    "    plt.show()\n"
   ],
   "id": "7af6ef0fe55302cb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. 使用 Autoencoder 進行異常檢測\n",
   "id": "48f4e3f68035b7dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:43:37.192345Z",
     "start_time": "2024-10-30T08:43:37.176343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_reconstruction_error(autoencoder, X):\n",
    "    \"\"\"\n",
    "    計算每個樣本的重建誤差（MSE）。\n",
    "\n",
    "    Args:\n",
    "        autoencoder (tensorflow.keras.Model): 已訓練的 Autoencoder 模型。\n",
    "        X (np.ndarray): 資料，形狀為 (樣本數, max_len, n_mels, 1)。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 每個樣本的 MSE，形狀為 (樣本數,)。\n",
    "    \"\"\"\n",
    "    reconstructions = autoencoder.predict(X)\n",
    "    mse = np.mean(np.power(X - reconstructions, 2), axis=(1,2,3))\n",
    "    return mse\n",
    "\n",
    "def determine_threshold(mse, percentile=95):\n",
    "    \"\"\"\n",
    "    根據重建誤差的百分位數設定閾值。\n",
    "\n",
    "    Args:\n",
    "        mse (np.ndarray): 重建誤差，形狀為 (樣本數,)。\n",
    "        percentile (float): 百分位數，用於設定閾值。\n",
    "\n",
    "    Returns:\n",
    "        float: 設定的閾值。\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(mse, percentile)\n",
    "    return threshold\n",
    "\n",
    "    "
   ],
   "id": "2658a06f4a9db7e7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:43:37.656265Z",
     "start_time": "2024-10-30T08:43:37.646804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_anomaly(mse, threshold):\n",
    "    \"\"\"\n",
    "    根據重建誤差和閾值預測是否為異常。\n",
    "\n",
    "    Args:\n",
    "        mse (np.ndarray): 重建誤差，形狀為 (樣本數,)。\n",
    "        threshold (float): 重建誤差閾值。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 預測標籤，1 表示異常，0 表示正常。\n",
    "    \"\"\"\n",
    "    return (mse > threshold).astype(int)\n"
   ],
   "id": "e22d58d5e793c935",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:43:38.031548Z",
     "start_time": "2024-10-30T08:43:38.024034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    評估模型的預測結果。\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): 真實標籤，形狀為 (樣本數,)。\n",
    "        y_pred (np.ndarray): 預測標籤，形狀為 (樣本數,)。\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n"
   ],
   "id": "a31efe8f53e94c0f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:45:00.732751Z",
     "start_time": "2024-10-30T08:43:38.366220Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # 訓練資料預處理（僅使用正常資料）\n",
    "X_train = create_normal_dataset('training_data')\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "print(input_shape)"
   ],
   "id": "77144cb141ef9e4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總正常樣本數: 10641\n",
      "特徵形狀（未正規化）: (10641, 192, 128)\n",
      "特徵形狀（擴展後）: (10641, 192, 128, 1)\n",
      "(192, 128, 1)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:45:37.197421Z",
     "start_time": "2024-10-30T08:45:36.979805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 建立 Autoencoder 模型\n",
    "autoencoder = create_autoencoder(input_shape)\n",
    "autoencoder.summary()"
   ],
   "id": "439f404e79a5e3c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 192, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 192, 128, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 192, 128, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 96, 64, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 96, 64, 32)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 96, 64, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 96, 64, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 48, 32, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 48, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 48, 32, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 48, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 24, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 49152)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               6291584   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 49152)             6340608   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 24, 16, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 24, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSampling  (None, 48, 32, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 48, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 48, 32, 64)        73792     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 48, 32, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " up_sampling2d_4 (UpSampling  (None, 96, 64, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 96, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 96, 64, 32)        18464     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 96, 64, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 192, 128, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 192, 128, 1)       289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,983,873\n",
      "Trainable params: 12,982,721\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-30T08:45:38.773906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 訓練 Autoencoder\n",
    "history = train_autoencoder(autoencoder, X_train, epochs=150, batch_size=32, validation_split=0.1)"
   ],
   "id": "48796801c346f82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "300/300 [==============================] - 23s 72ms/step - loss: 0.4036 - val_loss: 0.9754 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "300/300 [==============================] - 21s 71ms/step - loss: 0.1276 - val_loss: 0.7367 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "300/300 [==============================] - 21s 71ms/step - loss: 0.0968 - val_loss: 0.6709 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "300/300 [==============================] - 21s 71ms/step - loss: 0.0831 - val_loss: 0.5586 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "300/300 [==============================] - 21s 71ms/step - loss: 0.0749 - val_loss: 0.5387 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "300/300 [==============================] - 22s 72ms/step - loss: 0.0704 - val_loss: 0.5815 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "300/300 [==============================] - 22s 72ms/step - loss: 0.0654 - val_loss: 0.5397 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "300/300 [==============================] - 22s 73ms/step - loss: 0.0619 - val_loss: 0.5942 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "300/300 [==============================] - 22s 73ms/step - loss: 0.0595 - val_loss: 0.5557 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "300/300 [==============================] - 22s 73ms/step - loss: 0.0579 - val_loss: 0.5387 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "300/300 [==============================] - 22s 72ms/step - loss: 0.0560 - val_loss: 0.5435 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "300/300 [==============================] - 22s 72ms/step - loss: 0.0542 - val_loss: 0.5070 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "300/300 [==============================] - 21s 72ms/step - loss: 0.0529 - val_loss: 0.5397 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "300/300 [==============================] - 21s 72ms/step - loss: 0.0520 - val_loss: 0.4289 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "300/300 [==============================] - 22s 73ms/step - loss: 0.0500 - val_loss: 0.4028 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "300/300 [==============================] - 22s 74ms/step - loss: 0.0503 - val_loss: 0.3788 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "300/300 [==============================] - 23s 75ms/step - loss: 0.0491 - val_loss: 0.3739 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "300/300 [==============================] - 23s 75ms/step - loss: 0.0466 - val_loss: 0.2836 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "300/300 [==============================] - 23s 76ms/step - loss: 0.0490 - val_loss: 0.2565 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "300/300 [==============================] - 23s 75ms/step - loss: 0.0457 - val_loss: 0.2672 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "300/300 [==============================] - 23s 75ms/step - loss: 0.0451 - val_loss: 0.3006 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "300/300 [==============================] - 23s 76ms/step - loss: 0.0452 - val_loss: 0.2683 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "300/300 [==============================] - 22s 74ms/step - loss: 0.0459 - val_loss: 0.3094 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "300/300 [==============================] - 23s 76ms/step - loss: 0.0438 - val_loss: 0.2808 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "300/300 [==============================] - 23s 76ms/step - loss: 0.0439 - val_loss: 0.2373 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "300/300 [==============================] - 23s 76ms/step - loss: 0.0429 - val_loss: 0.2583 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "300/300 [==============================] - 23s 76ms/step - loss: 0.0458 - val_loss: 0.2135 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "300/300 [==============================] - 23s 76ms/step - loss: 0.0438 - val_loss: 0.1687 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "300/300 [==============================] - 23s 76ms/step - loss: 0.0420 - val_loss: 0.2055 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "300/300 [==============================] - 23s 76ms/step - loss: 0.0425 - val_loss: 0.1758 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "300/300 [==============================] - 24s 79ms/step - loss: 0.0416 - val_loss: 0.1722 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "300/300 [==============================] - 24s 78ms/step - loss: 0.0409 - val_loss: 0.1546 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0398 - val_loss: 0.1607 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0398 - val_loss: 0.1353 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0396 - val_loss: 0.1402 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0395 - val_loss: 0.1317 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0393 - val_loss: 0.1266 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0378 - val_loss: 0.1424 - lr: 1.0000e-04\n",
      "Epoch 39/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0397 - val_loss: 0.1249 - lr: 1.0000e-04\n",
      "Epoch 40/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0395 - val_loss: 0.1353 - lr: 1.0000e-04\n",
      "Epoch 41/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0381 - val_loss: 0.1905 - lr: 1.0000e-04\n",
      "Epoch 42/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0387 - val_loss: 0.1303 - lr: 1.0000e-04\n",
      "Epoch 43/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0388 - val_loss: 0.1319 - lr: 1.0000e-04\n",
      "Epoch 44/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0384 - val_loss: 0.1365 - lr: 1.0000e-04\n",
      "Epoch 45/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0365 - val_loss: 0.1568 - lr: 1.0000e-04\n",
      "Epoch 46/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0382 - val_loss: 0.1544 - lr: 1.0000e-04\n",
      "Epoch 47/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0368 - val_loss: 0.1439 - lr: 1.0000e-04\n",
      "Epoch 48/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0366 - val_loss: 0.1500 - lr: 1.0000e-04\n",
      "Epoch 49/150\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.0378\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0378 - val_loss: 0.2228 - lr: 1.0000e-04\n",
      "Epoch 50/150\n",
      "300/300 [==============================] - 23s 78ms/step - loss: 0.0369 - val_loss: 0.2018 - lr: 5.0000e-05\n",
      "Epoch 51/150\n",
      "300/300 [==============================] - 23s 77ms/step - loss: 0.0342 - val_loss: 0.1724 - lr: 5.0000e-05\n",
      "Epoch 52/150\n",
      "157/300 [==============>...............] - ETA: 10s - loss: 0.0376"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 繪製訓練過程\n",
    "plot_training_history(history)"
   ],
   "id": "15923d742da62d7e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
